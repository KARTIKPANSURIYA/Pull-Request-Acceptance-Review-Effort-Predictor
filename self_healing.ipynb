{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0778ff-6431-4843-b0d9-294f81880a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 2 â€” RQ2 (Self-Healing): Review Effort Prediction\n",
    "\n",
    "This notebook is **self-healing**: if your review-comments target is missing, it reconstructs it from the raw files.\n",
    "\n",
    "**What it does**\n",
    "1. **Robust load** with a local cache (fixes timeouts from synced drives).\n",
    "2. **Rebuild `n_review_comments`** by mapping review rows to PRs (via `commit_id â†’ sha â†’ pr_id`, and as a fallback using `pull_request_url`).\n",
    "3. **Type-safe merge** into your modeling table.\n",
    "4. Feature engineering (titles, churn, repo stats, simple task type).\n",
    "5. **RQ2a**: Predict number of review comments.\n",
    "6. **RQ2b**: Predict time-to-merge (hours).\n",
    "7. Pooled, group-wise (agent vs human), and balanced (downsample agent to human).\n",
    "\n",
    "> If your paths differ, change `BASE_PATH` in the next cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae868e17-4b07-4c72-ba4a-6c5b2c2525b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE_PATH: /Users/kartik/Desktop/aidev-phase2\n",
      "DATA: /Users/kartik/Desktop/aidev-phase2/data\n",
      "OUT: /Users/kartik/Desktop/aidev-phase2/outputs\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "BASE_PATH = Path(\"/Users/kartik/Desktop/aidev-phase2\")  # <--- EDIT if needed\n",
    "DATA = BASE_PATH / \"data\"\n",
    "OUT  = BASE_PATH / \"outputs\"\n",
    "OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"BASE_PATH:\", BASE_PATH)\n",
    "print(\"DATA:\", DATA)\n",
    "print(\"OUT:\", OUT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1e38170-3ee7-4cb7-a586-648319b08156",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "CACHE = Path(\"/tmp/aidev_phase2_cache\")\n",
    "CACHE.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def first_existing(paths):\n",
    "    for p in paths:\n",
    "        if p.exists():\n",
    "            return p\n",
    "    return None\n",
    "\n",
    "def cached_chunk_read_csv(src_path: Path, usecols, chunksize=500_000):\n",
    "    \"\"\"\n",
    "    Copy to local cache if needed, then read in chunks keeping only `usecols`.\n",
    "    \"\"\"\n",
    "    local_copy = CACHE / src_path.name\n",
    "    if (not local_copy.exists()) or (os.path.getmtime(local_copy) < os.path.getmtime(src_path)):\n",
    "        print(f\"Copying to local cache: {local_copy}\")\n",
    "        shutil.copy2(src_path, local_copy)\n",
    "    else:\n",
    "        print(f\"Using cached file: {local_copy}\")\n",
    "\n",
    "    chunks = []\n",
    "    try:\n",
    "        for ck in pd.read_csv(local_copy, low_memory=True, chunksize=chunksize,\n",
    "                              usecols=lambda c: c in usecols):\n",
    "            chunks.append(ck)\n",
    "    except TypeError:\n",
    "        for ck in pd.read_csv(local_copy, low_memory=True, chunksize=chunksize):\n",
    "            chunks.append(ck[usecols])\n",
    "\n",
    "    df = pd.concat(chunks, ignore_index=True)\n",
    "    df.columns = [c.strip().lower().replace(\" \", \"_\") for c in df.columns]\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "45414954-1367-4951-86ae-13be857cf08c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded FIXED: /Users/kartik/Desktop/aidev-phase2/outputs/modeling_table_fixed.csv\n",
      "df shape: (939409, 19)\n",
      "   pr_id      repo_id                                              title  \\\n",
      "0   1688  843988367.0  `metta code` --> `metta clip` and additional p...   \n",
      "1     41  992063465.0  feat: Comprehensive ruff error resolution with...   \n",
      "\n",
      "                                                body   state  \\\n",
      "0  Remove unused `root_key` variable to fix ruff ...  closed   \n",
      "1  ## ðŸŽ¯ Mission Accomplished: 100% Ruff Error Res...    open   \n",
      "\n",
      "             created_at             merged_at             closed_at  \\\n",
      "0  2025-07-25T18:15:36Z  2025-07-25T19:17:23Z  2025-07-25T19:17:23Z   \n",
      "1  2025-07-25T18:17:57Z                   NaN                   NaN   \n",
      "\n",
      "         agent  loc_added  loc_deleted  files_touched  stars  forks  y_accept  \\\n",
      "0  Claude_Code        0.0          0.0            0.0   72.0   32.0       1.0   \n",
      "1  Claude_Code        0.0          0.0            0.0    0.0    0.0       NaN   \n",
      "\n",
      "   hours_to_merge contrib_type  n_review_comments  first_comment_at  \n",
      "0        1.029722        agent                0.0               NaN  \n",
      "1             NaN        agent                0.0               NaN  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "fixed_csv = OUT / \"modeling_table_fixed.csv\"\n",
    "combo_csv = OUT / \"modeling_table_combined.csv\"\n",
    "\n",
    "if fixed_csv.exists():\n",
    "    df = pd.read_csv(fixed_csv, low_memory=False)\n",
    "    src_used = fixed_csv\n",
    "    print(\"Loaded FIXED:\", fixed_csv)\n",
    "elif combo_csv.exists():\n",
    "    df = pd.read_csv(combo_csv, low_memory=False)\n",
    "    src_used = combo_csv\n",
    "    print(\"Loaded COMBINED:\", combo_csv)\n",
    "else:\n",
    "    raise FileNotFoundError(\"Missing outputs/modeling_table_fixed.csv and modeling_table_combined.csv\")\n",
    "\n",
    "print(\"df shape:\", df.shape)\n",
    "print(df.head(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6f340940-43dd-4b6c-9987-c4817746acb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Normalize key columns correctly (Series -> .str.strip) ---\n",
    "for c in [\"commit_id\", \"original_commit_id\", \"pull_request_url\"]:\n",
    "    if c in rev.columns:\n",
    "        # robust: coerce to pandas 'string' dtype, strip whitespace; keep NaN as <NA>\n",
    "        rev[c] = rev[c].astype(\"string\").str.strip()\n",
    "\n",
    "diff = diff.dropna().drop_duplicates()\n",
    "\n",
    "# sha could be mixed/float; force to string and strip\n",
    "diff[\"sha\"] = diff[\"sha\"].astype(\"string\").str.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "47816fd2-1d14-42c5-b18b-2175d15e6710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed PR IDs for review rows: 19169 of 19670\n",
      "Non-zero review PRs after merge: 0\n",
      "count    939409.0\n",
      "mean          0.0\n",
      "std           0.0\n",
      "min           0.0\n",
      "25%           0.0\n",
      "50%           0.0\n",
      "75%           0.0\n",
      "max           0.0\n",
      "Name: n_review_comments, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Map commit_id -> pr_id via sha\n",
    "rev_join = rev.merge(diff, left_on=\"commit_id\", right_on=\"sha\", how=\"left\", suffixes=(\"\",\"_sha1\"))\n",
    "\n",
    "# Fallback: original_commit_id\n",
    "if \"original_commit_id\" in rev.columns:\n",
    "    rev_join = rev_join.merge(diff, left_on=\"original_commit_id\", right_on=\"sha\", how=\"left\", suffixes=(\"\",\"_sha2\"))\n",
    "\n",
    "# Prefer commit match; fallback to original_commit_id match\n",
    "pr_from_sha = rev_join[\"pr_id\"].copy()\n",
    "if \"pr_id_sha2\" in rev_join.columns:\n",
    "    pr_from_sha = pr_from_sha.fillna(rev_join[\"pr_id_sha2\"])\n",
    "\n",
    "# Fallback: extract /pull/<num> from pull_request_url\n",
    "import numpy as np, re, pandas as pd\n",
    "prnum_from_url = pd.Series(index=rev_join.index, dtype=\"Int64\")\n",
    "if \"pull_request_url\" in rev_join.columns:\n",
    "    def extract_prnum(url):\n",
    "        m = re.search(r\"/pull/(\\\\d+)\", str(url))\n",
    "        return int(m.group(1)) if m else None\n",
    "    prnum_from_url = pd.Series([extract_prnum(u) for u in rev_join[\"pull_request_url\"]], dtype=\"Int64\")\n",
    "\n",
    "# Final reconstructed pr_id (Int64)\n",
    "rev_join[\"pr_id_rec\"] = pd.to_numeric(pr_from_sha, errors=\"coerce\").astype(\"Int64\")\n",
    "rev_join.loc[rev_join[\"pr_id_rec\"].isna(), \"pr_id_rec\"] = prnum_from_url\n",
    "print(\"Reconstructed PR IDs for review rows:\", rev_join[\"pr_id_rec\"].notna().sum(), \"of\", len(rev_join))\n",
    "\n",
    "# Aggregate comments per PR\n",
    "if \"created_at\" in rev_join.columns:\n",
    "    agg_rev = (\n",
    "        rev_join.dropna(subset=[\"pr_id_rec\"])\n",
    "                .groupby(\"pr_id_rec\")\n",
    "                .agg(n_review_comments=(\"id\",\"count\"),\n",
    "                     first_comment_at=(\"created_at\",\"min\"))\n",
    "                .reset_index()\n",
    "                .rename(columns={\"pr_id_rec\":\"pr_id\"})\n",
    "    )\n",
    "else:\n",
    "    agg_rev = (\n",
    "        rev_join.dropna(subset=[\"pr_id_rec\"])\n",
    "                .groupby(\"pr_id_rec\")\n",
    "                .agg(n_review_comments=(\"id\",\"count\"))\n",
    "                .reset_index()\n",
    "                .rename(columns={\"pr_id_rec\":\"pr_id\"})\n",
    "    )\n",
    "    agg_rev[\"first_comment_at\"] = pd.NaT\n",
    "\n",
    "# Merge with consistent dtype\n",
    "df['pr_id']       = pd.to_numeric(df['pr_id'], errors='coerce').astype('Int64')\n",
    "agg_rev['pr_id']  = pd.to_numeric(agg_rev['pr_id'], errors='coerce').astype('Int64')\n",
    "\n",
    "df = df.drop(columns=[c for c in [\"n_review_comments\",\"first_comment_at\"] if c in df.columns], errors=\"ignore\")\n",
    "df = df.merge(agg_rev, on=\"pr_id\", how=\"left\", validate=\"m:1\")\n",
    "df[\"n_review_comments\"] = pd.to_numeric(df[\"n_review_comments\"], errors=\"coerce\").fillna(0)\n",
    "\n",
    "print(\"Non-zero review PRs after merge:\", (df[\"n_review_comments\"] > 0).sum())\n",
    "print(df[\"n_review_comments\"].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2888b68-c368-4da6-8985-ea349e780661",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

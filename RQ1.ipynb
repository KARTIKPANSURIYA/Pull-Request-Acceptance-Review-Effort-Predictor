{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e95967c-7d83-451b-91c2-5acf49c99835",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character 'â€“' (U+2013) (844362078.py, line 26)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[2], line 26\u001b[0;36m\u001b[0m\n\u001b[0;31m    - Accuracy, Precision, Recall, F1, ROCâ€“AUC\u001b[0m\n\u001b[0m                                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character 'â€“' (U+2013)\n"
     ]
    }
   ],
   "source": [
    "# RQ1 â€“ Predicting Pull Request Acceptance from Submission-Time Features\n",
    "\n",
    "This notebook implements the **RQ1 pipeline** for the project:\n",
    "\n",
    "> **RQ1:** How well can early PR signals (e.g., description clarity, diff size, files changed) predict acceptance or rejection?\n",
    "\n",
    "It is designed to be:\n",
    "- **Fully reproducible**\n",
    "- **Heavily documented**\n",
    "- **Easy to read for new contributors**\n",
    "\n",
    "We:\n",
    "\n",
    "1. Load a **row-per-PR feature table** (precomputed from the AIDev dataset).\n",
    "2. Select **submission-time features only** (no leakage).\n",
    "3. Train and evaluate **5 ML models**:\n",
    "   - Logistic Regression\n",
    "   - Random Forest\n",
    "   - Gradient Boosting\n",
    "   - Extra Trees\n",
    "   - MLP (Simple neural network)\n",
    "4. Add **baselines**:\n",
    "   - Majority classifier\n",
    "   - Simple heuristic rule\n",
    "5. Run **5-fold stratified cross-validation** and compute:\n",
    "   - Accuracy, Precision, Recall, F1, ROCâ€“AUC\n",
    "6. Produce a **confusion matrix** for the best model and extract false positives / false negatives for qualitative analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bb5ab07-51cc-4bac-b3e0-26ce21bd5cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 0. Imports and Global Configuration\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# Scikit-learn utilities\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier,\n",
    "    GradientBoostingClassifier,\n",
    "    ExtraTreesClassifier\n",
    ")\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    confusion_matrix\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14fdda42-4513-4967-a295-962ed32570b8",
   "metadata": {},
   "source": [
    "## 1. Load Feature Table\n",
    "\n",
    "We assume that Phase 2 already produced a **feature table** with:\n",
    "\n",
    "- One row per pull request (PR)\n",
    "- One column for the **binary acceptance label** (e.g., `label_merged` where 1 = merged, 0 = closed)\n",
    "- Multiple columns for **submission-time features**, such as:\n",
    "  - Text features: body length, title length, presence of URLs, etc.\n",
    "  - Diff features: lines added, lines deleted, files touched, churn ratio, etc.\n",
    "  - Repo features: stars, forks, language, etc.\n",
    "  - Temporal features: weekday, hour of creation, etc.\n",
    "\n",
    "ðŸ‘‰ **You must edit the file path and column names below to match your actual dataset.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cdf1d9ae-bc17-42b5-bc34-b943b057c037",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/aiddev_pr_features.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# ============================================================\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# 1. Load the feature table\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# ============================================================\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# TODO: change this to your actual CSV or Parquet file\u001b[39;00m\n\u001b[1;32m      6\u001b[0m DATA_PATH \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/aiddev_pr_features.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# <- EDIT THIS\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(DATA_PATH)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, df\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     11\u001b[0m df\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[1;32m   1881\u001b[0m     f,\n\u001b[1;32m   1882\u001b[0m     mode,\n\u001b[1;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[1;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1889\u001b[0m )\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    874\u001b[0m             handle,\n\u001b[1;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[1;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    879\u001b[0m         )\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/aiddev_pr_features.csv'"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 1. Load the feature table\n",
    "# ============================================================\n",
    "\n",
    "# TODO: change this to your actual CSV or Parquet file\n",
    "DATA_PATH = Path(\"data/aiddev_pr_features.csv\")  # <- EDIT THIS\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "print(\"Shape:\", df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "15f9b298-97ab-40b0-ac48-71faf7b300d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2) Utility: train/test split + metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cea34f1-4ae8-47c7-977b-a15440229407",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_xy(X, y, test_size=0.3, stratify=True, seed=42):\n",
    "    return train_test_split(X, y, test_size=test_size, random_state=seed, stratify=y if (stratify and len(np.unique(y))==2) else None)\n",
    "\n",
    "def cls_report(y_true, y_pred, y_prob=None):\n",
    "    out = dict(\n",
    "        acc = accuracy_score(y_true, y_pred),\n",
    "        f1  = f1_score(y_true, y_pred)\n",
    "    )\n",
    "    if y_prob is not None:\n",
    "        out['auc'] = roc_auc_score(y_true, y_prob)\n",
    "    return out\n",
    "\n",
    "def reg_report(y_true, y_pred):\n",
    "    return dict(\n",
    "        mae = mean_absolute_error(y_true, y_pred),\n",
    "        rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0d47accd-18eb-4848-8a6f-dd8c31d2efb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3) RQ1 â€” Acceptance (Merged vs Closed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0b9b69a8-21e7-48e1-b2f4-6a3aa49d2681",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3a) Pooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40734a9b-193e-43be-b07a-d7cdbf381b81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group</th>\n",
       "      <th>model</th>\n",
       "      <th>acc</th>\n",
       "      <th>f1</th>\n",
       "      <th>auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pooled</td>\n",
       "      <td>LogReg</td>\n",
       "      <td>0.771806</td>\n",
       "      <td>0.866579</td>\n",
       "      <td>0.617020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pooled</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.821682</td>\n",
       "      <td>0.900686</td>\n",
       "      <td>0.557339</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    group   model       acc        f1       auc\n",
       "0  pooled  LogReg  0.771806  0.866579  0.617020\n",
       "1  pooled      RF  0.821682  0.900686  0.557339"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr, X_te, y_tr, y_te = split_xy(X_rq1, y_rq1)\n",
    "\n",
    "# Logistic (class_weight helps with imbalance within pooled data)\n",
    "lr = LogisticRegression(max_iter=1000, class_weight='balanced', n_jobs=None)\n",
    "lr.fit(X_tr, y_tr)\n",
    "p_lr = lr.predict(X_te); proba_lr = lr.predict_proba(X_te)[:,1]\n",
    "pooled_lr = cls_report(y_te, p_lr, proba_lr)\n",
    "\n",
    "# Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=400, random_state=42, class_weight='balanced')\n",
    "rf.fit(X_tr, y_tr)\n",
    "p_rf = rf.predict(X_te); proba_rf = rf.predict_proba(X_te)[:,1]\n",
    "pooled_rf = cls_report(y_te, p_rf, proba_rf)\n",
    "\n",
    "pooled_rq1 = pd.DataFrame([dict(group='pooled', model='LogReg', **pooled_lr),\n",
    "                           dict(group='pooled', model='RF', **pooled_rf)])\n",
    "pooled_rq1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5b8a7f65-d2e4-4648-b423-1bc799e694db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3b) Per-group (agent / human)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b434efc9-ff11-469a-b8a6-2609364bd69c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group</th>\n",
       "      <th>model</th>\n",
       "      <th>acc</th>\n",
       "      <th>f1</th>\n",
       "      <th>auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pooled</td>\n",
       "      <td>LogReg</td>\n",
       "      <td>0.771806</td>\n",
       "      <td>0.866579</td>\n",
       "      <td>0.617020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pooled</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.821682</td>\n",
       "      <td>0.900686</td>\n",
       "      <td>0.557339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>agent</td>\n",
       "      <td>LogReg</td>\n",
       "      <td>0.771423</td>\n",
       "      <td>0.866336</td>\n",
       "      <td>0.617380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>agent</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.821989</td>\n",
       "      <td>0.900885</td>\n",
       "      <td>0.557868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>human</td>\n",
       "      <td>LogReg</td>\n",
       "      <td>0.615176</td>\n",
       "      <td>0.735666</td>\n",
       "      <td>0.577230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>human</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.783740</td>\n",
       "      <td>0.873694</td>\n",
       "      <td>0.588314</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    group   model       acc        f1       auc\n",
       "0  pooled  LogReg  0.771806  0.866579  0.617020\n",
       "1  pooled      RF  0.821682  0.900686  0.557339\n",
       "2   agent  LogReg  0.771423  0.866336  0.617380\n",
       "3   agent      RF  0.821989  0.900885  0.557868\n",
       "4   human  LogReg  0.615176  0.735666  0.577230\n",
       "5   human      RF  0.783740  0.873694  0.588314"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def run_group_cls(mask):\n",
    "    idx = df_rq1[mask.loc[df_rq1.index]].index\n",
    "    if len(idx) < 300: return None  # skip tiny groups\n",
    "    Xg, yg = X_rq1.loc[idx], y_rq1.loc[idx]\n",
    "    X_tr, X_te, y_tr, y_te = split_xy(Xg, yg)\n",
    "\n",
    "    lr = LogisticRegression(max_iter=1000, class_weight='balanced')\n",
    "    lr.fit(X_tr, y_tr)\n",
    "    p = lr.predict(X_te); proba = lr.predict_proba(X_te)[:,1]\n",
    "    row_lr = dict(model='LogReg', **cls_report(y_te, p, proba))\n",
    "\n",
    "    rf = RandomForestClassifier(n_estimators=400, random_state=42, class_weight='balanced')\n",
    "    rf.fit(X_tr, y_tr)\n",
    "    p = rf.predict(X_te); proba = rf.predict_proba(X_te)[:,1]\n",
    "    row_rf = dict(model='RF', **cls_report(y_te, p, proba))\n",
    "    return pd.DataFrame([row_lr, row_rf])\n",
    "\n",
    "agent_rows = run_group_cls(is_agent)\n",
    "if agent_rows is not None:\n",
    "    agent_rows.insert(0, 'group', 'agent')\n",
    "\n",
    "human_rows = run_group_cls(is_human)\n",
    "if human_rows is not None:\n",
    "    human_rows.insert(0, 'group', 'human')\n",
    "\n",
    "rq1_groups = pd.concat([pooled_rq1, agent_rows, human_rows], ignore_index=True)\n",
    "rq1_groups.to_csv(OUT/\"rq1_metrics_groups.csv\", index=False)\n",
    "rq1_groups\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "76159df3-b3e3-43d7-9107-d2955e99cb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3c) Balanced comparison (downsample agents to human count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29d513d8-308e-4609-b9a1-629a49698b2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group</th>\n",
       "      <th>model</th>\n",
       "      <th>acc</th>\n",
       "      <th>f1</th>\n",
       "      <th>auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>balanced(agent=human)</td>\n",
       "      <td>LogReg</td>\n",
       "      <td>0.87290</td>\n",
       "      <td>0.932137</td>\n",
       "      <td>0.570903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>balanced(agent=human)</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.86748</td>\n",
       "      <td>0.928955</td>\n",
       "      <td>0.609358</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   group   model      acc        f1       auc\n",
       "0  balanced(agent=human)  LogReg  0.87290  0.932137  0.570903\n",
       "1  balanced(agent=human)      RF  0.86748  0.928955  0.609358"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# ---- 3c) Balanced (downsample agents to human count) â€” with imputation ----\n",
    "rq1_df = df_rq1[['contrib_type']].copy()\n",
    "rq1_df['y'] = y_rq1.values\n",
    "rq1_df = pd.concat([rq1_df, X_rq1.reset_index(drop=True)], axis=1)\n",
    "\n",
    "human_subset = rq1_df[rq1_df['contrib_type']=='human']\n",
    "agent_subset = rq1_df[rq1_df['contrib_type']=='agent']\n",
    "\n",
    "if len(human_subset) > 300 and len(agent_subset) > 300:\n",
    "    agent_down = resample(agent_subset, n_samples=len(human_subset), replace=False, random_state=42)\n",
    "    balanced = pd.concat([human_subset, agent_down], ignore_index=True)\n",
    "\n",
    "    # Features/labels\n",
    "    Xb = balanced[X.columns].copy()\n",
    "    yb = balanced['y'].astype(int)\n",
    "\n",
    "    # replace inf -> NaN, then impute\n",
    "    Xb = Xb.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    # split\n",
    "    X_tr, X_te, y_tr, y_te = split_xy(Xb, yb)\n",
    "\n",
    "    # Pipelines with imputer\n",
    "    lr_pipe = make_pipeline(SimpleImputer(strategy='median'),\n",
    "                            LogisticRegression(max_iter=1000))\n",
    "    rf_pipe = make_pipeline(SimpleImputer(strategy='median'),\n",
    "                            RandomForestClassifier(n_estimators=400, random_state=42))\n",
    "\n",
    "    # fit/predict\n",
    "    lr_pipe.fit(X_tr, y_tr)\n",
    "    p = lr_pipe.predict(X_te); proba = lr_pipe.predict_proba(X_te)[:,1]\n",
    "    bal_lr = cls_report(y_te, p, proba)\n",
    "\n",
    "    rf_pipe.fit(X_tr, y_tr)\n",
    "    p = rf_pipe.predict(X_te); proba = rf_pipe.predict_proba(X_te)[:,1]\n",
    "    bal_rf = cls_report(y_te, p, proba)\n",
    "\n",
    "    rq1_bal = pd.DataFrame([\n",
    "        dict(group='balanced(agent=human)', model='LogReg', **bal_lr),\n",
    "        dict(group='balanced(agent=human)', model='RF', **bal_rf)\n",
    "    ])\n",
    "    rq1_bal.to_csv(OUT/\"rq1_metrics_balanced.csv\", index=False)\n",
    "    display(rq1_bal)\n",
    "else:\n",
    "    print(\"Not enough human or agent samples for balanced comparison.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "249d64b1-e9f0-4b38-9968-3b8c2e400a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3d) Feature importance (pooled RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d2d2f7a-779c-40f8-abc6-98b9b842f86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_full = RandomForestClassifier(n_estimators=600, random_state=42, class_weight='balanced').fit(X_rq1, y_rq1)\n",
    "imps = pd.Series(rf_full.feature_importances_, index=X_rq1.columns).sort_values(ascending=False).head(20)\n",
    "imps.to_csv(OUT/\"feature_importance_acceptance_top20.csv\")\n",
    "ax = imps.plot(kind='bar', figsize=(10,4)); ax.set_title(\"Top-20 Feature Importances (Acceptance)\")\n",
    "plt.tight_layout(); plt.savefig(OUT/\"feature_importance_acceptance_top20.png\", dpi=200)\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e9d88fa-2c8d-42bd-9562-ef87b7e0499b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 25\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# ------------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# 1) Define feature columns and targets (adapt to your DataFrame)\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# ------------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Example: everything except label columns is a feature\u001b[39;00m\n\u001b[1;32m     24\u001b[0m label_cols \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel_merged\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreview_comments\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime_to_merge_hours\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 25\u001b[0m feature_cols \u001b[38;5;241m=\u001b[39m [c \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;28;01mif\u001b[39;00m c \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m label_cols]\n\u001b[1;32m     27\u001b[0m X \u001b[38;5;241m=\u001b[39m df[feature_cols]\n\u001b[1;32m     28\u001b[0m y_rq1 \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel_merged\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)          \u001b[38;5;66;03m# 0 = closed, 1 = merged\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,\n",
    "    mean_absolute_error, mean_squared_error, r2_score\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier,\n",
    "    RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1) Define feature columns and targets (adapt to your DataFrame)\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "# Example: everything except label columns is a feature\n",
    "label_cols = [\"label_merged\", \"review_comments\", \"time_to_merge_hours\"]\n",
    "feature_cols = [c for c in df.columns if c not in label_cols]\n",
    "\n",
    "X = df[feature_cols]\n",
    "y_rq1 = df[\"label_merged\"].astype(int)          # 0 = closed, 1 = merged\n",
    "y_rq2_comments = df[\"review_comments\"].astype(float)\n",
    "y_rq2_ttm = df[\"time_to_merge_hours\"].astype(float)\n",
    "\n",
    "# Split numeric / categorical if you have them (simple version: all numeric)\n",
    "numeric_features = feature_cols\n",
    "categorical_features = []   # fill if you have categorical\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(), numeric_features),\n",
    "        # (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_features),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa884720-97be-44b7-be06-6a3cb1af6a91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

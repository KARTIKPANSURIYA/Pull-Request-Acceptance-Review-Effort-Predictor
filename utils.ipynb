{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "746002ae-771f-435e-9dde-29796d702dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded all_pull_request.csv: 932791 rows, 14 cols\n",
      "Loaded pr_commit_details.csv: 711923 rows, 14 cols\n",
      "Loaded pr_review_comments.csv: 19450 rows, 15 cols\n",
      "Loaded all_repository.csv: 116211 rows, 7 cols\n",
      "Loaded pr_comments.csv: 39122 rows, 7 cols\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd, numpy as np, re\n",
    "\n",
    "BASE = Path(\"/Users/kartik/Desktop/aidev-phase2\")\n",
    "DATA = BASE / \"data\"\n",
    "OUT  = BASE / \"outputs\"\n",
    "OUT.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "def load_csv(p):\n",
    "    # robust loader; prevents dtype warnings\n",
    "    df = pd.read_csv(p, low_memory=False)\n",
    "    df.columns = [c.strip().lower().replace(\" \", \"_\") for c in df.columns]\n",
    "    print(f\"Loaded {p.name}: {len(df)} rows, {len(df.columns)} cols\")\n",
    "    return df\n",
    "\n",
    "# paths\n",
    "PULLS   = DATA / \"all_pull_requests\" / \"all_pull_request.csv\"\n",
    "REPOS   = DATA / \"all_repositories\" / \"all_repository.csv\"\n",
    "COMMITS = DATA / \"pr_commit_details\" / \"pr_commit_details.csv\"\n",
    "REVCMTS = DATA / \"pr_review_comments\" / \"pr_review_comments.csv\"\n",
    "COMMENTS = DATA / \"pr_comments\" / \"pr_comments.csv\"\n",
    "\n",
    "pr        = load_csv(PULLS)\n",
    "diff      = load_csv(COMMITS)\n",
    "rev       = load_csv(REVCMTS)\n",
    "repo      = load_csv(REPOS)\n",
    "pr_comnts = load_csv(COMMENTS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b0d7313-5729-497e-a0ff-b7c27453665b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PR columns: ['id', 'pr_id', 'title', 'user', 'user_id', 'state', 'created_at', 'closed_at', 'merged_at', 'repo_url', 'repo_id', 'html_url', 'body', 'agent']\n",
      "DIFF columns: ['sha', 'pr_id', 'author', 'committer', 'message', 'commit_stats_total', 'commit_stats_additions', 'commit_stats_deletions', 'file_path', 'status', 'lines_added', 'lines_deleted', 'changes', 'patch']\n",
      "REV columns: ['pr_id', 'pull_request_review_id', 'user', 'user_type', 'diff_hunk', 'path', 'position', 'original_position', 'commit_id', 'original_commit_id', 'comment_body', 'pull_request_url', 'created_at', 'updated_at', 'in_reply_to_id']\n",
      "REPO columns: ['repo_id', 'url', 'license', 'full_name', 'language', 'forks', 'stars']\n"
     ]
    }
   ],
   "source": [
    "def smart_rename(df, mapping):\n",
    "    \"\"\"mapping: {'target_col': ['candidate1','candidate2',...]}\"\"\"\n",
    "    for target, candidates in mapping.items():\n",
    "        for c in candidates:\n",
    "            if c in df.columns:\n",
    "                if target != c:\n",
    "                    df.rename(columns={c: target}, inplace=True)\n",
    "                break  # stop at first match\n",
    "\n",
    "# PR table\n",
    "smart_rename(pr, {\n",
    "    'pr_id':        ['pr_id','pull_request_id','number','id'],\n",
    "    'repo_id':      ['repo_id','repository_id'],\n",
    "    'author_type':  ['author_type','contributor_type','creator_type'],\n",
    "    'title':        ['title'],\n",
    "    'body':         ['body','description'],\n",
    "    'state':        ['state','status'],\n",
    "    'created_at':   ['created_at','created'],\n",
    "    'merged_at':    ['merged_at','merged'],\n",
    "    'closed_at':    ['closed_at','closed'],\n",
    "})\n",
    "\n",
    "# Commit/diff table\n",
    "smart_rename(diff, {\n",
    "    'pr_id':         ['pr_id','pull_request_id','number','id'],\n",
    "    'lines_added':   ['lines_added','additions','added'],\n",
    "    'lines_deleted': ['lines_deleted','deletions','deleted','removed'],\n",
    "    'file_path':     ['file_path','filepath','file','filename','path','new_path','old_path']\n",
    "})\n",
    "\n",
    "# Review comments table\n",
    "smart_rename(rev, {\n",
    "    'pr_id':        ['pr_id','pull_request_id','number','id'],\n",
    "    'comment_body': ['comment_body','body','text','comment'],\n",
    "    'created_at':   ['created_at','created']\n",
    "})\n",
    "\n",
    "# Repository table\n",
    "smart_rename(repo, {\n",
    "    'repo_id':    ['repo_id','id'],\n",
    "    'stars':      ['stargazers_count','stars'],\n",
    "    'forks':      ['forks_count','forks']\n",
    "})\n",
    "\n",
    "# ensure expected columns exist\n",
    "print(\"PR columns:\", pr.columns.tolist())\n",
    "print(\"DIFF columns:\", diff.columns.tolist())\n",
    "print(\"REV columns:\", rev.columns.tolist())\n",
    "print(\"REPO columns:\", repo.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7941975d-5d85-4d24-a168-f8b16d65245c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['lines_added','lines_deleted']:\n",
    "    if col in diff.columns:\n",
    "        diff[col] = pd.to_numeric(diff[col], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "if 'file_path' in diff.columns:\n",
    "    diff['file_path'] = diff['file_path'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e804740a-03ef-472f-acad-0be7e4490677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: /Users/kartik/Desktop/aidev-phase2/outputs/full_join.csv rows: 932791\n"
     ]
    }
   ],
   "source": [
    "# choose a file-identity column for counting touched files\n",
    "file_key = 'file_path' if 'file_path' in diff.columns else None\n",
    "\n",
    "agg_diff_specs = {\n",
    "    'loc_added':   ('lines_added','sum')   if 'lines_added'   in diff.columns else (None, None),\n",
    "    'loc_deleted': ('lines_deleted','sum') if 'lines_deleted' in diff.columns else (None, None),\n",
    "    'files_touched': (file_key, 'nunique') if file_key else ('pr_id','size')\n",
    "}\n",
    "\n",
    "# build aggregation dict only for available columns\n",
    "agg_dict = {}\n",
    "if agg_diff_specs['loc_added'][0]:   agg_dict['loc_added']   = agg_diff_specs['loc_added'][1]\n",
    "if agg_diff_specs['loc_deleted'][0]: agg_dict['loc_deleted'] = agg_diff_specs['loc_deleted'][1]\n",
    "# Pandas needs mapping per column name → function; do via .agg on a dict of Series\n",
    "grouped = diff.groupby('pr_id')\n",
    "agg_pieces = {}\n",
    "if 'lines_added' in diff.columns:\n",
    "    agg_pieces['loc_added'] = grouped['lines_added'].sum()\n",
    "if 'lines_deleted' in diff.columns:\n",
    "    agg_pieces['loc_deleted'] = grouped['lines_deleted'].sum()\n",
    "if file_key:\n",
    "    agg_pieces['files_touched'] = grouped[file_key].nunique()\n",
    "else:\n",
    "    agg_pieces['files_touched'] = grouped['pr_id'].size()\n",
    "\n",
    "agg_diff = pd.DataFrame(agg_pieces).reset_index()\n",
    "\n",
    "# review comments aggregation (robust)\n",
    "agg_rev = (rev.groupby('pr_id')\n",
    "             .agg(n_review_comments=('pr_id','size'),\n",
    "                  first_comment_at=('created_at','min'))\n",
    "             .reset_index())\n",
    "\n",
    "# merge to full\n",
    "full = (pr\n",
    "        .merge(agg_diff, on='pr_id', how='left')\n",
    "        .merge(agg_rev,  on='pr_id', how='left'))\n",
    "\n",
    "if not repo.empty and 'repo_id' in full.columns and set(['stars','forks']).issubset(repo.columns):\n",
    "    full = full.merge(repo[['repo_id','stars','forks']], on='repo_id', how='left')\n",
    "\n",
    "# fill NAs for numeric aggregates\n",
    "for col in ['loc_added','loc_deleted','files_touched','n_review_comments','stars','forks']:\n",
    "    if col in full.columns:\n",
    "        full[col] = pd.to_numeric(full[col], errors='coerce').fillna(0)\n",
    "\n",
    "full.to_csv(OUT/'full_join.csv', index=False)\n",
    "print(\"✅ Saved:\", OUT/'full_join.csv', \"rows:\", len(full))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8acdd59-0e8b-4c8a-96d2-8ee362a1d8d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       y_accept  hours_to_merge\n",
      "count  859927.0   790139.000000\n",
      "mean        0.0        2.751494\n",
      "std         0.0       31.554555\n",
      "min         0.0        0.000278\n",
      "25%         0.0        0.003611\n",
      "50%         0.0        0.009167\n",
      "75%         0.0        0.051667\n",
      "max         0.0     2546.475556\n"
     ]
    }
   ],
   "source": [
    "# Acceptance label\n",
    "state = full.get('state','').astype(str).str.lower()\n",
    "full['y_accept'] = np.where(state.eq('merged'), 1,\n",
    "                    np.where(state.eq('closed'), 0, np.nan))\n",
    "\n",
    "# Time to merge\n",
    "def to_hours(row):\n",
    "    try:\n",
    "        if pd.notna(row['merged_at']) and pd.notna(row['created_at']):\n",
    "            return (pd.to_datetime(row['merged_at']) - pd.to_datetime(row['created_at'])).total_seconds()/3600.0\n",
    "    except Exception: pass\n",
    "    return np.nan\n",
    "\n",
    "if {'created_at','merged_at'}.issubset(full.columns):\n",
    "    full['hours_to_merge'] = full.apply(to_hours, axis=1)\n",
    "\n",
    "print(full[['y_accept','hours_to_merge']].describe(include='all'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2dacc2ac-a326-4f27-bce6-90cffaa5db5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State sample: state\n",
      "closed    859927\n",
      "open       72864\n",
      "Name: count, dtype: int64\n",
      "# merged_at notna: 790139\n",
      "# closed_at notna: 859927\n",
      "# created_at notna: 932791\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "OUT = Path(\"/Users/kartik/Desktop/aidev-phase2/outputs\")\n",
    "full = pd.read_csv(OUT/\"full_join.csv\", low_memory=False)\n",
    "\n",
    "print(\"State sample:\", full['state'].astype(str).str.lower().value_counts().head(10))\n",
    "print(\"# merged_at notna:\", full['merged_at'].notna().sum())\n",
    "print(\"# closed_at notna:\", full['closed_at'].notna().sum())\n",
    "print(\"# created_at notna:\", full['created_at'].notna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ee8124b-bb14-4c86-8880-29d5d151e619",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "y_accept\n",
       "1.0    790139\n",
       "NaN     72864\n",
       "0.0     69788\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalize state text\n",
    "state_norm = full['state'].astype(str).str.lower()\n",
    "\n",
    "# acceptance label:\n",
    "# 1 if merged_at exists\n",
    "# 0 if closed (state says closed/declined) OR closed_at exists but merged_at is null\n",
    "# NaN if still open (state=open OR both closed_at/merged_at null)\n",
    "full['y_accept'] = np.where(full['merged_at'].notna(), 1,\n",
    "                    np.where(\n",
    "                        (state_norm.isin(['closed','declined','rejected'])) | \n",
    "                        ((full['closed_at'].notna()) & (full['merged_at'].isna())),\n",
    "                        0,\n",
    "                        np.nan\n",
    "                    ))\n",
    "\n",
    "# recompute hours_to_merge using timestamps robustly\n",
    "def to_hours(row):\n",
    "    try:\n",
    "        ca = pd.to_datetime(row['created_at'])\n",
    "        ma = pd.to_datetime(row['merged_at'])\n",
    "        if pd.notna(ca) and pd.notna(ma):\n",
    "            return (ma - ca).total_seconds() / 3600.0\n",
    "    except Exception:\n",
    "        pass\n",
    "    return np.nan\n",
    "\n",
    "full['hours_to_merge'] = full.apply(to_hours, axis=1)\n",
    "\n",
    "full[['y_accept','hours_to_merge']].describe(include='all')\n",
    "full['y_accept'].value_counts(dropna=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d78b6bd-fe0a-48dd-86ea-fe5a8b280727",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "contrib_type\n",
       "unknown    932791\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# contrib_type from 'agent' column (which may be bool, 0/1, \"true\"/\"false\")\n",
    "s = full['agent'].astype(str).str.strip().str.lower()\n",
    "is_agent = s.isin(['true','1','yes'])\n",
    "is_human = s.isin(['false','0','no'])\n",
    "\n",
    "full['contrib_type'] = np.where(is_agent, 'agent',\n",
    "                         np.where(is_human, 'human', 'unknown'))\n",
    "\n",
    "full['contrib_type'].value_counts(dropna=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31c2bc35-d705-48cb-b9d7-1d43c58b7a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If later you want 'hybrid' via comment text hints\n",
    "if 'comment_body' in full.columns:\n",
    "    hybrid_hint = full['comment_body'].astype(str).str.contains(r'\\b(agent|copilot|ai)\\b', case=False, regex=True)\n",
    "    full.loc[(full['contrib_type']=='human') & hybrid_hint, 'contrib_type'] = 'hybrid'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37e1047c-c07f-4f8c-a1c4-dea090e00005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved corrected: /Users/kartik/Desktop/aidev-phase2/outputs/modeling_table.csv\n",
      "Label distribution (including NaN):\n",
      "y_accept\n",
      "1.0    790139\n",
      "NaN     72864\n",
      "0.0     69788\n",
      "Contrib types:\n",
      "contrib_type\n",
      "unknown    932791\n"
     ]
    }
   ],
   "source": [
    "full.to_csv(OUT/\"modeling_table.csv\", index=False)\n",
    "print(\"✅ Saved corrected:\", OUT/\"modeling_table.csv\")\n",
    "\n",
    "print(\"Label distribution (including NaN):\")\n",
    "print(full['y_accept'].value_counts(dropna=False).to_string())\n",
    "\n",
    "print(\"Contrib types:\")\n",
    "print(full['contrib_type'].value_counts(dropna=False).to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4673c997-aa92-4e97-bed6-9a3529c4a986",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>state</th>\n",
       "      <th>closed</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y_accept</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>69788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>790139</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "state     closed\n",
       "y_accept        \n",
       "0.0        69788\n",
       "1.0       790139"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sanity cross-tab\n",
    "pd.crosstab(full['y_accept'], full['state'].astype(str).str.lower()).head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "97f14db4-e6b9-406d-afe9-7020995ebde4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "agent\n",
       "OpenAI_Codex    814522\n",
       "Copilot          50447\n",
       "Cursor           32941\n",
       "Devin            29744\n",
       "Claude_Code       5137\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full['agent'].value_counts(dropna=False).head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c7a74ec-298d-41eb-8a2a-a01e77af2d00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "contrib_type\n",
       "agent    932791\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert and normalize agent column\n",
    "s = full['agent'].astype(str).str.strip().str.lower()\n",
    "\n",
    "# AI keywords (from your dataset)\n",
    "ai_tools = ['codex', 'copilot', 'cursor', 'devin', 'claude', 'openai', 'ai']\n",
    "\n",
    "def classify_agent(val):\n",
    "    if any(tool in val for tool in ai_tools):\n",
    "        return 'agent'\n",
    "    elif val in ['nan', 'none', '', 'null']:\n",
    "        return 'human'\n",
    "    else:\n",
    "        return 'human'  # fallback for unknown cases\n",
    "\n",
    "full['contrib_type'] = s.apply(classify_agent)\n",
    "full['contrib_type'].value_counts(dropna=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8c4dfe6f-2864-4a35-9de9-8083aeb01ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded human PRs: /Users/kartik/Desktop/aidev-phase2/data/human_pull_request/human_pull_request.csv (6618, 13)\n",
      "\n",
      "Label distribution (incl. NaN):\n",
      "y_accept\n",
      "1.0    795220\n",
      "NaN     73333\n",
      "0.0     70856\n",
      "\n",
      "Contributor types:\n",
      "contrib_type\n",
      "agent    932791\n",
      "human      6618\n",
      "\n",
      "✅ Saved: /Users/kartik/Desktop/aidev-phase2/outputs/modeling_table_combined.csv\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd, numpy as np, re\n",
    "\n",
    "BASE = Path(\"/Users/kartik/Desktop/aidev-phase2\")\n",
    "DATA = BASE / \"data\"\n",
    "OUT  = BASE / \"outputs\"\n",
    "OUT.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# 1) Load the human PR file\n",
    "HUMAN_PR_PATH = DATA / \"human_pull_request\" / \"human_pull_request.csv\"\n",
    "human_df = pd.read_csv(HUMAN_PR_PATH, low_memory=False)\n",
    "human_df.columns = [c.strip().lower().replace(\" \", \"_\") for c in human_df.columns]\n",
    "print(\"Loaded human PRs:\", HUMAN_PR_PATH, human_df.shape)\n",
    "\n",
    "# 2) Normalize columns to match your PR schema\n",
    "# PR columns you already have in the agent 'pr' table:\n",
    "# ['id','pr_id','title','user','user_id','state','created_at','closed_at','merged_at','repo_url','repo_id','html_url','body','agent']\n",
    "rename_map = {\n",
    "    'pull_request_id': 'pr_id',\n",
    "    'number':          'pr_id',\n",
    "    'id':              'id',\n",
    "    'repository_id':   'repo_id',\n",
    "    'repo':            'repo_id',\n",
    "    'status':          'state',\n",
    "    'description':     'body',\n",
    "    # timestamps:\n",
    "    'created':         'created_at',\n",
    "    'closed':          'closed_at',\n",
    "    'merged':          'merged_at',\n",
    "}\n",
    "for src, tgt in rename_map.items():\n",
    "    if src in human_df.columns and tgt not in human_df.columns:\n",
    "        human_df.rename(columns={src: tgt}, inplace=True)\n",
    "\n",
    "# Ensure the minimal set exists\n",
    "for col in ['pr_id','repo_id','title','body','state','created_at','merged_at','closed_at']:\n",
    "    if col not in human_df.columns:\n",
    "        human_df[col] = np.nan\n",
    "\n",
    "# 3) Tag as human (your agent file had explicit 'agent' names; here we force human)\n",
    "human_df['agent'] = 'human'            # for consistency with your agent column\n",
    "human_df['contrib_type'] = 'human'     # explicit contributor type\n",
    "\n",
    "# 4) Build/refresh diff & review aggregations across the GLOBAL diff/rev tables you already loaded\n",
    "#    (If those tables also include human PRs, they will join; if not, they’ll remain NaN which we fill to 0.)\n",
    "# Recompute aggregations to be safe (uses robust names from your DIFF/REV printouts):\n",
    "agg_diff = (diff\n",
    "            .groupby('pr_id')\n",
    "            .agg(loc_added=('lines_added','sum'),\n",
    "                 loc_deleted=('lines_deleted','sum'),\n",
    "                 files_touched=('file_path','nunique'))\n",
    "            .reset_index())\n",
    "\n",
    "agg_rev = (rev\n",
    "           .groupby('pr_id')\n",
    "           .agg(n_review_comments=('pr_id','size'),\n",
    "                first_comment_at=('created_at','min'))\n",
    "           .reset_index())\n",
    "\n",
    "# 5) Build a unified PR table: agent PRs (from full) + human PRs, then re-merge aggregations\n",
    "#    If you still have the earlier 'full' (agent-only) in memory, great.\n",
    "#    Otherwise reload it:\n",
    "agent_full = pd.read_csv(OUT / \"full_join.csv\", low_memory=False)\n",
    "\n",
    "# Strip down agent_full to PR-level cols (to avoid double-merging old aggregates)\n",
    "pr_agent_cols = ['pr_id','repo_id','title','body','state','created_at','merged_at','closed_at','agent']\n",
    "for c in pr_agent_cols:\n",
    "    if c not in agent_full.columns:\n",
    "        agent_full[c] = np.nan\n",
    "pr_agent = agent_full[pr_agent_cols].copy()\n",
    "\n",
    "# Stack agent + human PRs\n",
    "pr_all = pd.concat([pr_agent, human_df[pr_agent_cols]], ignore_index=True)\n",
    "\n",
    "# Re-merge diff/review/repo so both agent + human get features\n",
    "combined = (pr_all\n",
    "            .merge(agg_diff, on='pr_id', how='left')\n",
    "            .merge(agg_rev,  on='pr_id', how='left'))\n",
    "\n",
    "if not repo.empty and set(['repo_id','stars','forks']).issubset(repo.columns):\n",
    "    combined = combined.merge(repo[['repo_id','stars','forks']], on='repo_id', how='left')\n",
    "\n",
    "# Fill numeric NAs\n",
    "for col in ['loc_added','loc_deleted','files_touched','n_review_comments','stars','forks']:\n",
    "    if col in combined.columns:\n",
    "        combined[col] = pd.to_numeric(combined[col], errors='coerce').fillna(0)\n",
    "\n",
    "# 6) Labels (acceptance + effort) — timestamp-first logic\n",
    "state_norm = combined['state'].astype(str).str.lower()\n",
    "combined['y_accept'] = np.where(combined['merged_at'].notna(), 1,\n",
    "                        np.where(\n",
    "                            (state_norm.isin(['closed','declined','rejected'])) |\n",
    "                            ((combined['closed_at'].notna()) & (combined['merged_at'].isna())),\n",
    "                            0, np.nan))\n",
    "\n",
    "def to_hours(row):\n",
    "    try:\n",
    "        ca = pd.to_datetime(row['created_at'])\n",
    "        ma = pd.to_datetime(row['merged_at'])\n",
    "        if pd.notna(ca) and pd.notna(ma):\n",
    "            return (ma - ca).total_seconds()/3600.0\n",
    "    except Exception:\n",
    "        pass\n",
    "    return np.nan\n",
    "\n",
    "combined['hours_to_merge'] = combined.apply(to_hours, axis=1)\n",
    "\n",
    "# 7) contrib_type from 'agent' column (agent tools vs human)\n",
    "s = combined['agent'].astype(str).str.strip().str.lower()\n",
    "ai_tools = ['codex','copilot','cursor','devin','claude','openai','ai']\n",
    "\n",
    "def classify_agent(val):\n",
    "    if any(tool in val for tool in ai_tools):\n",
    "        return 'agent'\n",
    "    elif val in ['human','nan','','none','null']:\n",
    "        return 'human'\n",
    "    else:\n",
    "        # be conservative: if not a known AI tool name, treat as human\n",
    "        return 'human'\n",
    "\n",
    "combined['contrib_type'] = s.apply(classify_agent)\n",
    "\n",
    "print(\"\\nLabel distribution (incl. NaN):\")\n",
    "print(combined['y_accept'].value_counts(dropna=False).to_string())\n",
    "\n",
    "print(\"\\nContributor types:\")\n",
    "print(combined['contrib_type'].value_counts(dropna=False).to_string())\n",
    "\n",
    "# 8) Save final modeling table\n",
    "combined.to_csv(OUT / \"modeling_table_combined.csv\", index=False)\n",
    "print(\"\\n✅ Saved:\", OUT / \"modeling_table_combined.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63abc261-82eb-4801-89e7-394cbcd86d03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
